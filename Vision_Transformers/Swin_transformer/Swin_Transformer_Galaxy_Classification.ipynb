{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5659a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ç¯å¢ƒè®¾ç½®å’ŒGPUé…ç½®\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import SwinForImageClassification, SwinConfig\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# æ£€æŸ¥GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUåç§°: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. æ•°æ®è·¯å¾„å’Œå‚æ•°è®¾ç½®\n",
    "DATA_PATH = \"/remote-home/cs_acmis_hby/Galaxy-Zoo-Classification/Contrast_experiment/Modern CNNs/Galaxy-Classification-Using-CNN/output_dataset\"\n",
    "\n",
    "# å®éªŒå‚æ•°\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # Swin Transformeræ¨èè¾ƒå°batch size\n",
    "NUM_CLASSES = 8\n",
    "EPOCHS = 50  # æ ¹æ®è®ºæ–‡è¡¨æ ¼ä¸­çš„è®¾ç½®\n",
    "LEARNING_RATE = 5e-5  # é¢„è®­ç»ƒæ¨¡å‹æ¨èå­¦ä¹ ç‡\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "# ç±»åˆ«åç§°\n",
    "CLASS_NAMES = [\n",
    "    'barred_spirals',\n",
    "    'cigar_shaped_elliptical', \n",
    "    'edge_on',\n",
    "    'in_between_elliptical',\n",
    "    'irregular',\n",
    "    'merger',\n",
    "    'round_elliptical',\n",
    "    'unbarred_spirals'\n",
    "]\n",
    "\n",
    "print(f\"ğŸ¯ å®éªŒé…ç½®:\")\n",
    "print(f\"   æ•°æ®è·¯å¾„: {DATA_PATH}\")\n",
    "print(f\"   å›¾åƒå¤§å°: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}\")\n",
    "print(f\"   ç±»åˆ«æ•°: {NUM_CLASSES}\")\n",
    "print(f\"   å­¦ä¹ ç‡: {LEARNING_RATE}\")\n",
    "print(f\"   æœ€å¤§è½®æ•°: {EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. æ•°æ®é›†ç±»å’Œæ•°æ®åŠ è½½\n",
    "class GalaxyDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # åŠ è½½æ‰€æœ‰å›¾åƒè·¯å¾„å’Œæ ‡ç­¾\n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_file in glob.glob(os.path.join(class_dir, '*.jpg')):\n",
    "                    self.images.append(img_file)\n",
    "                    self.labels.append(class_idx)\n",
    "        \n",
    "        print(f\"ğŸ“ {data_dir.split('/')[-1]} æ•°æ®åŠ è½½:\")\n",
    "        print(f\"   æ€»æ ·æœ¬æ•°: {len(self.images)}\")\n",
    "        \n",
    "        # ç»Ÿè®¡å„ç±»åˆ«æ ·æœ¬æ•°\n",
    "        for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "            count = self.labels.count(class_idx)\n",
    "            print(f\"   {class_name}: {count}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # åŠ è½½å›¾åƒ\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# æ•°æ®å˜æ¢\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomRotation(180),  # å¤©æ–‡å›¾åƒå¯ä»»æ„æ—‹è½¬\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "print(\"ğŸ”„ åˆ›å»ºæ•°æ®é›†...\")\n",
    "train_dataset = GalaxyDataset(os.path.join(DATA_PATH, 'train'), transform=train_transform)\n",
    "val_dataset = GalaxyDataset(os.path.join(DATA_PATH, 'val'), transform=val_test_transform)\n",
    "test_dataset = GalaxyDataset(os.path.join(DATA_PATH, 'test'), transform=val_test_transform)\n",
    "\n",
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"\\nâœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:\")\n",
    "print(f\"   è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}\")\n",
    "print(f\"   éªŒè¯æ‰¹æ¬¡: {len(val_loader)}\")\n",
    "print(f\"   æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206baf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. åŠ è½½å®˜æ–¹é¢„è®­ç»ƒSwin Transformeræ¨¡å‹\n",
    "print(\"ğŸ”§ åŠ è½½å®˜æ–¹Swin Transformeræ¨¡å‹...\")\n",
    "\n",
    "# ä½¿ç”¨Hugging Faceå®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    \"microsoft/swin-base-patch4-window7-224\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True  # å¿½ç•¥åˆ†ç±»å¤´å¤§å°ä¸åŒ¹é…\n",
    ").to(device)\n",
    "\n",
    "# æ¨¡å‹ä¿¡æ¯\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!\")\n",
    "print(f\"   æ¨¡å‹: microsoft/swin-base-patch4-window7-224\")\n",
    "print(f\"   æ€»å‚æ•°é‡: {total_params:,}\")\n",
    "print(f\"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "print(f\"   é¢„è®­ç»ƒ: ImageNet-22K â†’ ImageNet-1K\")\n",
    "print(f\"   è®ºæ–‡å¼•ç”¨: Liu et al. (2021) - Swin Transformer V1\")\n",
    "\n",
    "# è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-7)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"\\nğŸ“‹ è®­ç»ƒé…ç½®:\")\n",
    "print(f\"   ä¼˜åŒ–å™¨: AdamW\")\n",
    "print(f\"   å­¦ä¹ ç‡è°ƒåº¦: Cosine Annealing\")\n",
    "print(f\"   æŸå¤±å‡½æ•°: CrossEntropyLoss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. è®­ç»ƒå’ŒéªŒè¯å‡½æ•°\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).logits\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), 100.*correct/total\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs).logits\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "print(\"âœ… è®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e922e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. å¼€å§‹è®­ç»ƒ\n",
    "print(f\"ğŸš€ å¼€å§‹è®­ç»ƒå®˜æ–¹Swin Transformer...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
    "    \n",
    "    # è®­ç»ƒ\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # éªŒè¯\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # æ›´æ–°å­¦ä¹ ç‡\n",
    "    scheduler.step()\n",
    "    \n",
    "    # è®°å½•å†å²\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'swin_transformer_official_best.pth')\n",
    "        print(f\"âœ… æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"â¹ï¸ Early stopping after {patience} epochs without improvement\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… è®­ç»ƒå®Œæˆ!\")\n",
    "print(f\"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%\")\n",
    "print(f\"   å®é™…è®­ç»ƒè½®æ•°: {epoch + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. æµ‹è¯•é›†è¯„ä¼°\n",
    "print(\"ğŸ“Š æµ‹è¯•é›†è¯„ä¼°...\")\n",
    "\n",
    "# åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(torch.load('swin_transformer_classification_best.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs).logits\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100. * correct / total\n",
    "test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"\\nğŸ¯ æµ‹è¯•ç»“æœ:\")\n",
    "print(f\"   æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.2f}%\")\n",
    "print(f\"   æµ‹è¯•æŸå¤±: {test_loss:.4f}\")\n",
    "\n",
    "# è®¡ç®—F1åˆ†æ•°\n",
    "f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "print(f\"   Macro F1-Score: {f1_macro:.4f}\")\n",
    "print(f\"   Weighted F1-Score: {f1_weighted:.4f}\")\n",
    "\n",
    "# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
    "print(f\"\\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(classification_report(all_labels, all_predictions, \n",
    "                          target_names=CLASS_NAMES, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2fc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5a85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348e913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
